{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "os.chdir(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load final dataset\n",
    "\n",
    "comments_master = pd.read_csv(\"Final total CN dataset.csv\", engine='python')\n",
    "\n",
    "comments_master = comments_master.drop(columns=['主帖内容', '主帖类型', '主贴id', '作者',\n",
    "       '公司', '关注数', '关键词', '关键词指纹', '其他', '内容', '出生年份', '出生日期', '分析对象',\n",
    "       '分析对象ID', '原创内容', '双向关注数', '发布终端', '发表时间', '图片url', '在看数', '城市', '城市级别',\n",
    "       '头像url', '学校', '差评数', '微信公众号id', '微博数', '微博类型', '微博认证类型', '性别', '总互动量',\n",
    "       '情感', '搜索关键词', '收藏数', '数据类型', '文章摘要', '文章次序', '文章类型', '新闻来源', '星座',\n",
    "       '是否主贴', '是否广告', '是否推荐', '是否水军', '是否水贴', '是否火贴', '是否精华帖', '是否置顶', '更新时间',\n",
    "       '本文作者', '标题', '活跃度', '源内容', '源微博mid', '源微博用户id', '源微博用户名', '点赞数',\n",
    "       '爱好标签', '用户id', '用户主页url', '用户地址', '用户简介', '用户认证原因', '省份', '站点id',\n",
    "       '站点名称', '类别id', '粉丝数', '粉丝级别', '表情', '视频url', '认证类型', '评论数', '话题',\n",
    "       '转发微博mid', '转发数', '阅读数'])\n",
    "\n",
    "comments_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to remove URLs YAASS \n",
    "import re\n",
    "\n",
    "for i in range (len(comments_master)):\n",
    "    comments_master['Translated_discussion'].loc[i] = re.sub(r'https?://\\S+','',comments_master['Translated_discussion'].loc[i])\n",
    "    \n",
    "    comments_master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import EDA libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import ngrams\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "#libraries for n-grams and visualisation\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~–¬†�'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove quote + edit as that is how people reply to comments\n",
    "#remove less words than original stopwords set to preserve context\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words.add('cant')\n",
    "stop_words.add('could')\n",
    "stop_words.add('couldnt')\n",
    "stop_words.add('couldve')\n",
    "stop_words.add('id')\n",
    "stop_words.add('im')\n",
    "stop_words.add('ive')\n",
    "stop_words.add('dont')\n",
    "stop_words.add('didnt')\n",
    "stop_words.add('doesnt')\n",
    "stop_words.add('havent')\n",
    "stop_words.add('hes')\n",
    "stop_words.add('shes')\n",
    "stop_words.add('should')\n",
    "stop_words.add('shouldnt')\n",
    "stop_words.add('shouldve')\n",
    "stop_words.add('theres')\n",
    "stop_words.add('its')\n",
    "stop_words.add('thats')\n",
    "stop_words.add('weve')\n",
    "stop_words.add('whats')\n",
    "stop_words.add('wont')\n",
    "stop_words.add('wouldnt')\n",
    "stop_words.add('wouldve')\n",
    "stop_words.add('youll')\n",
    "stop_words.add('youre')\n",
    "\n",
    "stop_words.add('absolutely')\n",
    "stop_words.add('actually')\n",
    "stop_words.add('already')\n",
    "stop_words.add('always')\n",
    "stop_words.add('also')\n",
    "stop_words.add('although')\n",
    "stop_words.add('always')\n",
    "stop_words.add('another')\n",
    "stop_words.add('anyone')\n",
    "stop_words.add('anyway')\n",
    "stop_words.add('around')\n",
    "stop_words.add('ask')\n",
    "stop_words.add('back')\n",
    "stop_words.add('better')\n",
    "stop_words.add('bit')\n",
    "stop_words.add('either')\n",
    "stop_words.add('etc')\n",
    "stop_words.add('else')\n",
    "stop_words.add('even')\n",
    "stop_words.add('first')\n",
    "stop_words.add('go')\n",
    "stop_words.add('get')\n",
    "stop_words.add('give')\n",
    "stop_words.add('great')\n",
    "stop_words.add('especially')\n",
    "stop_words.add('hope')\n",
    "stop_words.add('hopefully')\n",
    "stop_words.add('however')\n",
    "stop_words.add('know')\n",
    "stop_words.add('like')\n",
    "stop_words.add('make')\n",
    "stop_words.add('maybe')\n",
    "stop_words.add('mean')\n",
    "stop_words.add('might')\n",
    "stop_words.add('mine')\n",
    "stop_words.add('please')\n",
    "stop_words.add('probably')\n",
    "stop_words.add('really')\n",
    "stop_words.add('say')\n",
    "stop_words.add('seem')\n",
    "stop_words.add('since')\n",
    "stop_words.add('sometimes')\n",
    "stop_words.add('suggest')\n",
    "stop_words.add('still')\n",
    "stop_words.add('take')\n",
    "stop_words.add('thank')\n",
    "stop_words.add('thing')\n",
    "stop_words.add('things')\n",
    "stop_words.add('think')\n",
    "stop_words.add('though')\n",
    "stop_words.add('u')\n",
    "stop_words.add('us')\n",
    "stop_words.add('use')\n",
    "stop_words.add('without')\n",
    "stop_words.add('would')\n",
    "stop_words.add('x')\n",
    "stop_words.add('yeah')\n",
    "stop_words.add('yet')\n",
    "\n",
    "stop_words.add('alright')\n",
    "stop_words.add('haha')\n",
    "stop_words.add('hehe')\n",
    "stop_words.add('hey')\n",
    "stop_words.add('hi')\n",
    "stop_words.add('lol')\n",
    "stop_words.add('ok')\n",
    "stop_words.add('okay')\n",
    "stop_words.add('omg')\n",
    "stop_words.add('yay')\n",
    "stop_words.add('yep')\n",
    "stop_words.add('yes')\n",
    "\n",
    "stop_words.add('across')\n",
    "stop_words.add('advise')\n",
    "stop_words.add('agree')\n",
    "stop_words.add('ago')\n",
    "stop_words.add('almost')\n",
    "stop_words.add('along')\n",
    "stop_words.add('answer')\n",
    "stop_words.add('anything')\n",
    "stop_words.add('anywhere')\n",
    "stop_words.add('apply')\n",
    "stop_words.add('appreciate')\n",
    "stop_words.add('available')\n",
    "stop_words.add('believe')\n",
    "stop_words.add('best')\n",
    "stop_words.add('better')\n",
    "stop_words.add('certainly')\n",
    "stop_words.add('chance')\n",
    "stop_words.add('change')\n",
    "stop_words.add('check')\n",
    "stop_words.add('choose')\n",
    "stop_words.add('come')\n",
    "stop_words.add('compare')\n",
    "stop_words.add('condition')\n",
    "stop_words.add('consider')\n",
    "stop_words.add('crossed')\n",
    "stop_words.add('currently')\n",
    "stop_words.add('decide')\n",
    "stop_words.add('definitely')\n",
    "stop_words.add('depend')\n",
    "stop_words.add('edit')\n",
    "stop_words.add('enjoy')\n",
    "stop_words.add('enough')\n",
    "stop_words.add('every')\n",
    "stop_words.add('everyone')\n",
    "stop_words.add('everything')\n",
    "stop_words.add('find')\n",
    "stop_words.add('fingers')\n",
    "stop_words.add('follow')\n",
    "stop_words.add('forward')\n",
    "stop_words.add('issue')\n",
    "stop_words.add('last')\n",
    "stop_words.add('later')\n",
    "stop_words.add('leave')\n",
    "stop_words.add('let')\n",
    "stop_words.add('likely')\n",
    "stop_words.add('list')\n",
    "stop_words.add('look')\n",
    "stop_words.add('love')\n",
    "stop_words.add('lovely')\n",
    "stop_words.add('luck')\n",
    "stop_words.add('mention')\n",
    "stop_words.add('nana')\n",
    "stop_words.add('need')\n",
    "stop_words.add('never')\n",
    "stop_words.add('nice')\n",
    "stop_words.add('next')\n",
    "stop_words.add('nothing')\n",
    "stop_words.add('offer')\n",
    "stop_words.add('often')\n",
    "stop_words.add('other')\n",
    "stop_words.add('option')\n",
    "stop_words.add('otherwise')\n",
    "stop_words.add('perhaps')\n",
    "stop_words.add('possible')\n",
    "stop_words.add('put')\n",
    "stop_words.add('quote')\n",
    "stop_words.add('rather')\n",
    "stop_words.add('regard')\n",
    "stop_words.add('remember')\n",
    "stop_words.add('result')\n",
    "stop_words.add('return')\n",
    "stop_words.add('review')\n",
    "stop_words.add('s')\n",
    "stop_words.add('several')\n",
    "stop_words.add('someone')\n",
    "stop_words.add('somewhere')\n",
    "stop_words.add('sure')\n",
    "stop_words.add('towards')\n",
    "stop_words.add('understand')\n",
    "stop_words.add('unless')\n",
    "stop_words.add('usually')\n",
    "stop_words.add('want')\n",
    "stop_words.add('well')\n",
    "stop_words.add('within')\n",
    "stop_words.add('year')\n",
    "stop_words.add('years')\n",
    "stop_words.add('am')\n",
    "stop_words.add('pm')\n",
    "stop_words.add('2021')\n",
    "\n",
    "pprint(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation.replace('/','').replace('-',''))).replace('/',' ').replace('-',' ').replace('\\n', ' ').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stop_words=stopwords):\n",
    "    return [word for word in text.split(' ') if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(word):\n",
    "    return lemma.lemmatize(word,'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_master['Discussion_cleaned'] = comments_master['Translated_discussion']\\\n",
    "                    .apply(lambda string: remove_stopwords(remove_punctuation(string), stop_words))\\\n",
    "                    .str.join(' ')\\\n",
    "                    .apply(lambda string: [lemmatize_text(word) for word in str(string).replace(',','').split()])\\\n",
    "                    .str.join(' ')\\\n",
    "                    .apply(lambda string: [str(i) for i in remove_stopwords(string, stop_words)])\n",
    "\n",
    "comments_master['Discussion_cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save whole dataset\n",
    "dataframe_for_csv = comments_master.copy(deep=True)\n",
    "\n",
    "dataframe_for_csv['Discussion_cleaned'] = dataframe_for_csv['Discussion_cleaned'].str.join(' ')\n",
    "\n",
    "dataframe_for_csv.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/dataset to segment.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 1 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build segment 1 dataset\n",
    "segment_1 = comments_master.loc[(comments_master['label_Events and Attractions>Bars & Restaurants']>0)|\n",
    "                                (comments_master['label_Events and Attractions>Nightclubs']>0)]\n",
    "\n",
    "#reset index\n",
    "segment_1 = segment_1.reset_index()\n",
    "\n",
    "segment_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataset for the segment into csv\n",
    "segment_1.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/segment_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams for Segment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all comments into one list of words for n-grams analysis\n",
    "for_ngrams1 = []\n",
    "\n",
    "for i in range(len(segment_1['Discussion_cleaned'])):\n",
    "    for x in segment_1['Discussion_cleaned'][i]:\n",
    "        for_ngrams1.append(x)\n",
    "\n",
    "for_ngrams1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_corpus1 = ' '.join(for_ngrams1)\n",
    "\n",
    "main_corpus1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq1 = Counter(for_ngrams1)\n",
    "common_words1 = word_freq1.most_common(20)\n",
    "\n",
    "common_words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words1.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words1_1 = list(zip(*common_words1))[0]\n",
    "score1_1 = list(zip(*common_words1))[1]\n",
    "y_pos1_1 = np.arange(len(words1_1))\n",
    "\n",
    "commonwords1a = list(words1_1)\n",
    "\n",
    "commonwords1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams1 = ngrams(for_ngrams1, 1)\n",
    "\n",
    "unigram_freq1 = Counter(unigrams1)\n",
    "\n",
    "common_unigrams1 = unigram_freq1.most_common(20)\n",
    "\n",
    "common_unigrams1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_for_csv = unigram_freq1\n",
    "all_words_for_csv = word_freq_for_csv.most_common(len(word_freq_for_csv))\n",
    "\n",
    "all_words_for_csv.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words_for_csv = list(zip(*all_words_for_csv))[0]\n",
    "score_for_csv = list(zip(*all_words_for_csv))[1]\n",
    "\n",
    "Term_frequency = pd.DataFrame(data={'term': words_for_csv, 'frequency': score_for_csv})\n",
    "\n",
    "Term_frequency.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/segment_1_unigrams.csv\")\n",
    "\n",
    "Term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos1_1, score1_1, align='center')\n",
    "ax.set_yticks(y_pos1_1)\n",
    "ax.set_yticklabels(words1_1)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring unigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud1 = WordCloud(width=2000, height=1400, max_words=100, max_font_size=300, background_color=\"white\", relative_scaling=.8).generate(main_corpus1)\n",
    "plt.imshow(wordcloud1, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams1 = ngrams(for_ngrams1, 2)\n",
    "\n",
    "bigram_freq1 = Counter(bigrams1)\n",
    "\n",
    "common_bigrams1 = bigram_freq1.most_common(20)\n",
    "\n",
    "common_bigrams1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_bigrams1.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words2_1 = list(zip(*common_bigrams1))[0]\n",
    "score2_1 = list(zip(*common_bigrams1))[1]\n",
    "y_pos2_1 = np.arange(len(words2_1))\n",
    "\n",
    "commonwords1b = list(words2_1)\n",
    "\n",
    "commonwords1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos2_1, score2_1, align='center')\n",
    "ax.set_yticks(y_pos2_1)\n",
    "ax.set_yticklabels(words2_1)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring bigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams1 = ngrams(for_ngrams1, 3)\n",
    "\n",
    "trigram_freq1 = Counter(trigrams1)\n",
    "\n",
    "common_trigrams1 = trigram_freq1.most_common(20)\n",
    "\n",
    "common_trigrams1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_trigrams1.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words3_1 = list(zip(*common_trigrams1))[0]\n",
    "score3_1 = list(zip(*common_trigrams1))[1]\n",
    "y_pos3_1 = np.arange(len(words3_1))\n",
    "\n",
    "commonwords1c = list(words3_1)\n",
    "\n",
    "commonwords1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos3_1, score3_1, align='center')\n",
    "ax.set_yticks(y_pos3_1)\n",
    "ax.set_yticklabels(words3_1)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring trigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 2 Analysis (Extreme Sports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build segment 2 dataset and reset index\n",
    "segment_2 = comments_master.loc[(comments_master['label_News and Politics>Weather']>0)|\n",
    "                                (comments_master['label_Sports>Diving']>0)|\n",
    "                                (comments_master['label_Sports>Skiing']>0)|\n",
    "                                (comments_master['label_Sports>Extreme Sports>Surfing and Bodyboarding']>0)|\n",
    "                                (comments_master['label_Sports>Swimming']>0)|\n",
    "                                (comments_master['label_Sports>Extreme Sports>Climbing']>0)|\n",
    "                                (comments_master['label_Sports>Extreme Sports>Scuba Diving']>0)|\n",
    "                                (comments_master['label_Sports>Extreme Sports>Canoeing and Kayaking']>0)|\n",
    "                                (comments_master['label_Travel>Travel Type>Adventure Travel']>0)].reset_index()\n",
    "\n",
    "segment_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataset for the segment into csv\n",
    "segment_2.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/segment_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all comments into one list of words for n-grams analysis\n",
    "for_ngrams2 = []\n",
    "\n",
    "for i in range(len(segment_2['Discussion_cleaned'])):\n",
    "    for x in segment_2['Discussion_cleaned'][i]:\n",
    "        for_ngrams2.append(x)\n",
    "\n",
    "for_ngrams2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_corpus2 = ' '.join(for_ngrams2)\n",
    "\n",
    "main_corpus2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq2 = Counter(for_ngrams2)\n",
    "common_words2 = word_freq2.most_common(20)\n",
    "\n",
    "common_words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words2.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words1_2 = list(zip(*common_words2))[0]\n",
    "score1_2 = list(zip(*common_words2))[1]\n",
    "y_pos1_2 = np.arange(len(words1_2))\n",
    "\n",
    "commonwords2a = list(words1_2)\n",
    "\n",
    "commonwords2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams2 = ngrams(for_ngrams2, 1)\n",
    "\n",
    "unigram_freq2 = Counter(unigrams2)\n",
    "\n",
    "unigram_freq2.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_for_csv = unigram_freq2\n",
    "all_words_for_csv = word_freq_for_csv.most_common(len(word_freq_for_csv))\n",
    "\n",
    "all_words_for_csv.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words_for_csv = list(zip(*all_words_for_csv))[0]\n",
    "score_for_csv = list(zip(*all_words_for_csv))[1]\n",
    "\n",
    "Term_frequency = pd.DataFrame(data={'term': words_for_csv, 'frequency': score_for_csv})\n",
    "\n",
    "Term_frequency.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/segment_2_unigrams.csv\")\n",
    "\n",
    "Term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos1_2, score1_2, align='center')\n",
    "ax.set_yticks(y_pos1_2)\n",
    "ax.set_yticklabels(words1_2)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring unigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud2 = WordCloud(width=2000, height=1400, max_words=100, max_font_size=300, background_color=\"white\", relative_scaling=.8).generate(main_corpus2)\n",
    "plt.imshow(wordcloud2, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams2 = ngrams(for_ngrams2, 2)\n",
    "\n",
    "bigram_freq2 = Counter(bigrams2)\n",
    "\n",
    "common_bigrams2 = bigram_freq2.most_common(20)\n",
    "\n",
    "common_bigrams2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_bigrams2.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words2_2 = list(zip(*common_bigrams2))[0]\n",
    "score2_2 = list(zip(*common_bigrams2))[1]\n",
    "y_pos2_2 = np.arange(len(words2_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos2_2, score2_2, align='center')\n",
    "ax.set_yticks(y_pos2_2)\n",
    "ax.set_yticklabels(words2_2)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring bigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams2 = ngrams(for_ngrams2, 3)\n",
    "\n",
    "trigram_freq2 = Counter(trigrams2)\n",
    "\n",
    "common_trigrams2 = trigram_freq2.most_common(20)\n",
    "\n",
    "common_trigrams2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_trigrams2.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words3_2 = list(zip(*common_trigrams2))[0]\n",
    "score3_2 = list(zip(*common_trigrams2))[1]\n",
    "y_pos3_2 = np.arange(len(words3_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos3_2, score3_2, align='center')\n",
    "ax.set_yticks(y_pos3_2)\n",
    "ax.set_yticklabels(words3_2)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring trigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 3 Analysis (City Break)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build segment 3 dataset and reset index\n",
    "segment_3 = comments_master.loc[(comments_master['label_Fine Art']>0)|\n",
    "                                (comments_master['label_Events and Attractions>Museums & Galleries']>0)|\n",
    "                                (comments_master['label_Events and Attractions>Zoos & Aquariums']>0)|\n",
    "                                (comments_master['label_Events and Attractions>Amusement and Theme Parks']>0)|\n",
    "                                (comments_master['label_Events and Attractions>Theater Venues and Events']>0)].reset_index()\n",
    "\n",
    "segment_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataset for the segment into csv\n",
    "segment_3.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/segment_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all comments into one list of words for n-grams analysis\n",
    "for_ngrams3 = []\n",
    "\n",
    "for i in range(len(segment_3['Discussion_cleaned'])):\n",
    "    for x in segment_3['Discussion_cleaned'][i]:\n",
    "        for_ngrams3.append(x)\n",
    "\n",
    "for_ngrams3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_corpus3 = ' '.join(for_ngrams3)\n",
    "\n",
    "main_corpus3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq3 = Counter(for_ngrams3)\n",
    "common_words3 = word_freq3.most_common(20)\n",
    "\n",
    "common_words3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words3.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words1_3 = list(zip(*common_words3))[0]\n",
    "score1_3 = list(zip(*common_words3))[1]\n",
    "y_pos1_3 = np.arange(len(words1_3))\n",
    "\n",
    "commonwords3 = list(words1_3)\n",
    "\n",
    "commonwords3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams3 = ngrams(for_ngrams3, 1)\n",
    "\n",
    "unigram_freq3 = Counter(unigrams3)\n",
    "\n",
    "unigram_freq3.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_for_csv = unigram_freq3\n",
    "all_words_for_csv = word_freq_for_csv.most_common(len(word_freq_for_csv))\n",
    "\n",
    "all_words_for_csv.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words_for_csv = list(zip(*all_words_for_csv))[0]\n",
    "score_for_csv = list(zip(*all_words_for_csv))[1]\n",
    "\n",
    "Term_frequency = pd.DataFrame(data={'term': words_for_csv, 'frequency': score_for_csv})\n",
    "\n",
    "Term_frequency.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/segment_3_unigrams.csv\")\n",
    "\n",
    "Term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos1_3, score1_3, align='center')\n",
    "ax.set_yticks(y_pos1_3)\n",
    "ax.set_yticklabels(words1_3)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring unigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud3 = WordCloud(width=2000, height=1400, max_words=100, max_font_size=300, background_color=\"white\", relative_scaling=.8).generate(main_corpus3)\n",
    "plt.imshow(wordcloud3, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams3 = ngrams(for_ngrams3, 2)\n",
    "\n",
    "bigram_freq3 = Counter(bigrams3)\n",
    "\n",
    "common_bigrams3 = bigram_freq3.most_common(20)\n",
    "\n",
    "common_bigrams3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_bigrams3.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words2_3 = list(zip(*common_bigrams3))[0]\n",
    "score2_3 = list(zip(*common_bigrams3))[1]\n",
    "y_pos2_3 = np.arange(len(words2_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos2_3, score2_3, align='center')\n",
    "ax.set_yticks(y_pos2_3)\n",
    "ax.set_yticklabels(words2_3)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring bigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams3 = ngrams(for_ngrams3, 3)\n",
    "\n",
    "trigram_freq3 = Counter(trigrams3)\n",
    "\n",
    "common_trigrams3 = trigram_freq3.most_common(20)\n",
    "\n",
    "common_trigrams3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_trigrams3.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words3_3 = list(zip(*common_trigrams3))[0]\n",
    "score3_3 = list(zip(*common_trigrams3))[1]\n",
    "y_pos3_3 = np.arange(len(words3_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos3_3, score3_3, align='center')\n",
    "ax.set_yticks(y_pos3_3)\n",
    "ax.set_yticklabels(words3_3)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring trigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 4 Analysis (Business Travel & Digital Nomads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#build segment 4 dataset and reset index\n",
    "segment_4 = comments_master.loc[(comments_master['label_Travel>Travel Type>Air Travel']>0)|\n",
    "                                (comments_master['label_Travel>Travel Type>Business Travel']>0)|\n",
    "                                (comments_master['label_Family and Relationships>Single Life']>0)].reset_index()\n",
    "\n",
    "segment_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataset for the segment into csv\n",
    "segment_4.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/segment_4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all comments into one list of words for n-grams analysis\n",
    "for_ngrams4 = []\n",
    "\n",
    "for i in range(len(segment_4['Discussion_cleaned'])):\n",
    "    for x in segment_4['Discussion_cleaned'][i]:\n",
    "        for_ngrams4.append(x)\n",
    "\n",
    "for_ngrams4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_corpus4 = ' '.join(for_ngrams4)\n",
    "\n",
    "main_corpus4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq4 = Counter(for_ngrams4)\n",
    "common_words4 = word_freq4.most_common(20)\n",
    "\n",
    "common_words4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words4.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words1_4 = list(zip(*common_words4))[0]\n",
    "score1_4 = list(zip(*common_words4))[1]\n",
    "y_pos1_4 = np.arange(len(words1_4))\n",
    "\n",
    "commonwords4 = list(words1_4)\n",
    "\n",
    "commonwords4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams4 = ngrams(for_ngrams4, 1)\n",
    "\n",
    "unigram_freq4 = Counter(unigrams4)\n",
    "\n",
    "unigram_freq4.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_for_csv = unigram_freq4\n",
    "all_words_for_csv = word_freq_for_csv.most_common(len(word_freq_for_csv))\n",
    "\n",
    "all_words_for_csv.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words_for_csv = list(zip(*all_words_for_csv))[0]\n",
    "score_for_csv = list(zip(*all_words_for_csv))[1]\n",
    "\n",
    "Term_frequency = pd.DataFrame(data={'term': words_for_csv, 'frequency': score_for_csv})\n",
    "\n",
    "Term_frequency.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/segment_4_unigrams.csv\")\n",
    "\n",
    "Term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos1_4, score1_4, align='center')\n",
    "ax.set_yticks(y_pos1_4)\n",
    "ax.set_yticklabels(words1_4)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring unigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud4 = WordCloud(width=2000, height=1400, max_words=100, max_font_size=300, background_color=\"white\", relative_scaling=.8).generate(main_corpus4)\n",
    "plt.imshow(wordcloud4, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams4 = ngrams(for_ngrams4, 2)\n",
    "\n",
    "bigram_freq4 = Counter(bigrams4)\n",
    "\n",
    "common_bigrams4 = bigram_freq4.most_common(20)\n",
    "\n",
    "common_bigrams4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_bigrams4.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "bigrams4 = list(zip(*common_bigrams4))[0]\n",
    "score2_4 = list(zip(*common_bigrams4))[1]\n",
    "y_pos2_4 = np.arange(len(bigrams4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos2_4, score2_4, align='center')\n",
    "ax.set_yticks(y_pos2_4)\n",
    "ax.set_yticklabels(bigrams4)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring bigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams4 = ngrams(for_ngrams4, 3)\n",
    "\n",
    "trigram_freq4 = Counter(trigrams4)\n",
    "\n",
    "common_trigrams4 = trigram_freq4.most_common(20)\n",
    "\n",
    "common_trigrams4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_trigrams4.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "trigrams4 = list(zip(*common_trigrams4))[0]\n",
    "score3_4 = list(zip(*common_trigrams4))[1]\n",
    "y_pos3_4 = np.arange(len(trigrams4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos3_4, score3_4, align='center')\n",
    "ax.set_yticks(y_pos3_4)\n",
    "ax.set_yticklabels(trigrams4)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring trigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 5 Analysis (Culinary Tourist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build segment 5 dataset and reset index\n",
    "segment_5 = comments_master.loc[(comments_master['label_Food & Drink>World Cuisines']>0)|\n",
    "                                (comments_master['label_Food & Drink>Cooking']>0)|\n",
    "                                (comments_master['label_Food & Drink>Barbecues and Grilling']>0)|\n",
    "                                (comments_master['label_Food & Drink>Vegetarian Diets']>0)].reset_index()\n",
    "\n",
    "segment_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataset for the segment into csv\n",
    "segment_5.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/segment_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all comments into one list of words for n-grams analysis\n",
    "for_ngrams5 = []\n",
    "\n",
    "for i in range(len(segment_5['Discussion_cleaned'])):\n",
    "    for x in segment_5['Discussion_cleaned'][i]:\n",
    "        for_ngrams5.append(x)\n",
    "\n",
    "for_ngrams5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_corpus5 = ' '.join(for_ngrams5)\n",
    "\n",
    "main_corpus5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq5 = Counter(for_ngrams5)\n",
    "common_words5 = word_freq5.most_common(20)\n",
    "\n",
    "common_words5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words5.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words1_5 = list(zip(*common_words5))[0]\n",
    "score1_5 = list(zip(*common_words5))[1]\n",
    "y_pos1_5 = np.arange(len(words1_5))\n",
    "\n",
    "commonwords5 = list(words1_5)\n",
    "\n",
    "commonwords5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams5 = ngrams(for_ngrams5, 1)\n",
    "\n",
    "unigram_freq5 = Counter(unigrams5)\n",
    "\n",
    "unigram_freq5.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_for_csv = unigram_freq5\n",
    "all_words_for_csv = word_freq_for_csv.most_common(len(word_freq_for_csv))\n",
    "\n",
    "all_words_for_csv.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words_for_csv = list(zip(*all_words_for_csv))[0]\n",
    "score_for_csv = list(zip(*all_words_for_csv))[1]\n",
    "\n",
    "Term_frequency = pd.DataFrame(data={'term': words_for_csv, 'frequency': score_for_csv})\n",
    "\n",
    "Term_frequency.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/segment_5_unigrams.csv\")\n",
    "\n",
    "Term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos1_5, score1_5, align='center')\n",
    "ax.set_yticks(y_pos1_5)\n",
    "ax.set_yticklabels(words1_5)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring unigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud5 = WordCloud(width=2000, height=1400, max_words=100, max_font_size=300, background_color=\"white\", relative_scaling=.8).generate(main_corpus5)\n",
    "plt.imshow(wordcloud5, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams5 = ngrams(for_ngrams5, 2)\n",
    "\n",
    "bigram_freq5 = Counter(bigrams5)\n",
    "\n",
    "common_bigrams5 = bigram_freq5.most_common(20)\n",
    "\n",
    "common_bigrams5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_bigrams5.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "bigrams5 = list(zip(*common_bigrams5))[0]\n",
    "score2_5 = list(zip(*common_bigrams5))[1]\n",
    "y_pos2_5 = np.arange(len(bigrams5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos2_5, score2_5, align='center')\n",
    "ax.set_yticks(y_pos2_5)\n",
    "ax.set_yticklabels(bigrams5)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring bigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams5 = ngrams(for_ngrams5, 3)\n",
    "\n",
    "trigram_freq5 = Counter(trigrams5)\n",
    "\n",
    "common_trigrams5 = trigram_freq5.most_common(20)\n",
    "\n",
    "common_trigrams5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_trigrams5.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "trigrams5 = list(zip(*common_trigrams5))[0]\n",
    "score3_5 = list(zip(*common_trigrams5))[1]\n",
    "y_pos3_5 = np.arange(len(trigrams5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos3_5, score3_5, align='center')\n",
    "ax.set_yticks(y_pos3_5)\n",
    "ax.set_yticklabels(trigrams5)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring trigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 8 Analysis (Short Family Trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build segment 8 dataset and reset index\n",
    "segment_8 = comments_master.loc[(comments_master['label_Travel>Travel Type>Family Travel']>0)|\n",
    "                                (comments_master['label_Travel>Travel Type>Day Trips']>0)|\n",
    "                                (comments_master['label_Events and Attractions>National & Civic Holidays']>0)].reset_index()\n",
    "\n",
    "segment_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataset for the segment into csv\n",
    "segment_8.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/segment_8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all comments into one list of words for n-grams analysis\n",
    "for_ngrams8 = []\n",
    "\n",
    "for i in range(len(segment_8['Discussion_cleaned'])):\n",
    "    for x in segment_8['Discussion_cleaned'][i]:\n",
    "        for_ngrams8.append(x)\n",
    "\n",
    "for_ngrams8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_corpus8 = ' '.join(for_ngrams8)\n",
    "\n",
    "main_corpus8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq8 = Counter(for_ngrams8)\n",
    "common_words8 = word_freq8.most_common(20)\n",
    "\n",
    "common_words8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words8.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words1_8 = list(zip(*common_words8))[0]\n",
    "score1_8 = list(zip(*common_words8))[1]\n",
    "y_pos1_8 = np.arange(len(words1_8))\n",
    "\n",
    "commonwords8 = list(words1_8)\n",
    "\n",
    "commonwords8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams8 = ngrams(for_ngrams8, 1)\n",
    "\n",
    "unigram_freq8 = Counter(unigrams8)\n",
    "\n",
    "unigram_freq8.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_for_csv = unigram_freq8\n",
    "all_words_for_csv = word_freq_for_csv.most_common(len(word_freq_for_csv))\n",
    "\n",
    "all_words_for_csv.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words_for_csv = list(zip(*all_words_for_csv))[0]\n",
    "score_for_csv = list(zip(*all_words_for_csv))[1]\n",
    "\n",
    "Term_frequency = pd.DataFrame(data={'term': words_for_csv, 'frequency': score_for_csv})\n",
    "\n",
    "Term_frequency.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/segment_8_unigrams.csv\")\n",
    "\n",
    "Term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos1_8, score1_8, align='center')\n",
    "ax.set_yticks(y_pos1_8)\n",
    "ax.set_yticklabels(words1_8)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring unigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud8 = WordCloud(width=2000, height=1400, max_words=100, max_font_size=300, background_color=\"white\", relative_scaling=.8).generate(main_corpus8)\n",
    "plt.imshow(wordcloud8, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams8 = ngrams(for_ngrams8, 2)\n",
    "\n",
    "bigram_freq8 = Counter(bigrams8)\n",
    "\n",
    "common_bigrams8 = bigram_freq8.most_common(20)\n",
    "\n",
    "common_bigrams8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_bigrams8.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "bigrams8 = list(zip(*common_bigrams8))[0]\n",
    "score2_8 = list(zip(*common_bigrams8))[1]\n",
    "y_pos2_8 = np.arange(len(bigrams8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos2_8, score2_8, align='center')\n",
    "ax.set_yticks(y_pos2_8)\n",
    "ax.set_yticklabels(bigrams8)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring bigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams8 = ngrams(for_ngrams8, 3)\n",
    "\n",
    "trigram_freq8 = Counter(trigrams8)\n",
    "\n",
    "common_trigrams8 = trigram_freq8.most_common(20)\n",
    "\n",
    "common_trigrams8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_trigrams8.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "trigrams8 = list(zip(*common_trigrams8))[0]\n",
    "score3_8 = list(zip(*common_trigrams8))[1]\n",
    "y_pos3_8 = np.arange(len(trigrams8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos3_8, score3_8, align='center')\n",
    "ax.set_yticks(y_pos3_8)\n",
    "ax.set_yticklabels(trigrams8)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring trigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 9 Analysis (Packaged Tours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build segment 9 dataset and reset index\n",
    "segment_9 = comments_master.loc[(comments_master['label_Travel>Travel Type>Cruises']>0)|\n",
    "                                (comments_master['label_Travel>Travel Type>Rail Travel']>0)|\n",
    "                                (comments_master['label_Sports>Martial Arts']>0)|\n",
    "                                (comments_master['label_Travel>Travel Type>Camping']>0)|\n",
    "                                (comments_master['label_Events and Attractions>Casinos & Gambling']>0)|\n",
    "                                (comments_master['label_Technology & Computing>Artificial Intelligence']>0)].reset_index()\n",
    "\n",
    "segment_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataset for the segment into csv\n",
    "segment_9.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/segment_9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all comments into one list of words for n-grams analysis\n",
    "for_ngrams9 = []\n",
    "\n",
    "for i in range(len(segment_9['Discussion_cleaned'])):\n",
    "    for x in segment_9['Discussion_cleaned'][i]:\n",
    "        for_ngrams9.append(x)\n",
    "\n",
    "for_ngrams9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_corpus9 = ' '.join(for_ngrams9)\n",
    "\n",
    "main_corpus9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq9 = Counter(for_ngrams9)\n",
    "common_words9 = word_freq9.most_common(20)\n",
    "\n",
    "common_words9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words9.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words1_9 = list(zip(*common_words9))[0]\n",
    "score1_9 = list(zip(*common_words9))[1]\n",
    "y_pos1_9 = np.arange(len(words1_9))\n",
    "\n",
    "commonwords9 = list(words1_9)\n",
    "\n",
    "commonwords9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams9 = ngrams(for_ngrams9, 1)\n",
    "\n",
    "unigram_freq9 = Counter(unigrams9)\n",
    "\n",
    "unigram_freq9.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_for_csv = unigram_freq9\n",
    "all_words_for_csv = word_freq_for_csv.most_common(len(word_freq_for_csv))\n",
    "\n",
    "all_words_for_csv.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words_for_csv = list(zip(*all_words_for_csv))[0]\n",
    "score_for_csv = list(zip(*all_words_for_csv))[1]\n",
    "\n",
    "Term_frequency = pd.DataFrame(data={'term': words_for_csv, 'frequency': score_for_csv})\n",
    "\n",
    "Term_frequency.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/segment_9_unigrams.csv\")\n",
    "\n",
    "Term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos1_9, score1_9, align='center')\n",
    "ax.set_yticks(y_pos1_9)\n",
    "ax.set_yticklabels(words1_9)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring unigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud9 = WordCloud(width=2000, height=1400, max_words=100, max_font_size=300, background_color=\"white\", relative_scaling=.8).generate(main_corpus9)\n",
    "plt.imshow(wordcloud9, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams9 = ngrams(for_ngrams9, 2)\n",
    "\n",
    "bigram_freq9 = Counter(bigrams9)\n",
    "\n",
    "common_bigrams9 = bigram_freq9.most_common(20)\n",
    "\n",
    "common_bigrams9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_bigrams9.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "bigrams9 = list(zip(*common_bigrams9))[0]\n",
    "score2_9 = list(zip(*common_bigrams9))[1]\n",
    "y_pos2_9 = np.arange(len(bigrams9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos2_9, score2_9, align='center')\n",
    "ax.set_yticks(y_pos2_9)\n",
    "ax.set_yticklabels(bigrams9)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring bigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams9 = ngrams(for_ngrams9, 3)\n",
    "\n",
    "trigram_freq9 = Counter(trigrams9)\n",
    "\n",
    "common_trigrams9 = trigram_freq9.most_common(20)\n",
    "\n",
    "common_trigrams9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_trigrams9.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "trigrams9 = list(zip(*common_trigrams9))[0]\n",
    "score3_9 = list(zip(*common_trigrams9))[1]\n",
    "y_pos3_9 = np.arange(len(trigrams9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos3_9, score3_9, align='center')\n",
    "ax.set_yticks(y_pos3_9)\n",
    "ax.set_yticklabels(trigrams9)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring trigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 10 Analysis (Relaxed Detox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build segment 10 dataset and reset index\n",
    "segment_10 = comments_master.loc[(comments_master['label_Food & Drink>Desserts and Baking']>0)|\n",
    "                                 (comments_master['label_Healthy Living>Fitness and Exercise']>0)|\n",
    "                                 (comments_master['label_Food & Drink>Non-Alcoholic Beverages']>0)|\n",
    "                                 (comments_master['label_Travel>Travel Type>Spas']>0)|\n",
    "                                 (comments_master['label_Hobbies & Interests>Musical Instruments']>0)|\n",
    "                                 (comments_master['label_Sports>Tennis']>0)].reset_index()\n",
    "\n",
    "segment_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataset for the segment into csv\n",
    "segment_10.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/segment_10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all comments into one list of words for n-grams analysis\n",
    "for_ngrams10 = []\n",
    "\n",
    "for i in range(len(segment_10['Discussion_cleaned'])):\n",
    "    for x in segment_10['Discussion_cleaned'][i]:\n",
    "        for_ngrams10.append(x)\n",
    "\n",
    "for_ngrams10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_corpus10 = ' '.join(for_ngrams10)\n",
    "\n",
    "main_corpus10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq10 = Counter(for_ngrams10)\n",
    "common_words10 = word_freq10.most_common(20)\n",
    "\n",
    "common_words10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words10.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words1_10 = list(zip(*common_words10))[0]\n",
    "score1_10 = list(zip(*common_words10))[1]\n",
    "y_pos1_10 = np.arange(len(words1_10))\n",
    "\n",
    "commonwords10 = list(words1_10)\n",
    "\n",
    "commonwords10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams10 = ngrams(for_ngrams10, 1)\n",
    "\n",
    "unigram_freq10 = Counter(unigrams10)\n",
    "\n",
    "unigram_freq10.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_for_csv = unigram_freq10\n",
    "all_words_for_csv = word_freq_for_csv.most_common(len(word_freq_for_csv))\n",
    "\n",
    "all_words_for_csv.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words_for_csv = list(zip(*all_words_for_csv))[0]\n",
    "score_for_csv = list(zip(*all_words_for_csv))[1]\n",
    "\n",
    "Term_frequency = pd.DataFrame(data={'term': words_for_csv, 'frequency': score_for_csv})\n",
    "\n",
    "Term_frequency.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/segment_10_unigrams.csv\")\n",
    "\n",
    "Term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos1_10, score1_10, align='center')\n",
    "ax.set_yticks(y_pos1_10)\n",
    "ax.set_yticklabels(words1_10)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring unigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud10 = WordCloud(width=2000, height=1400, max_words=100, max_font_size=300, background_color=\"white\", relative_scaling=.8).generate(main_corpus10)\n",
    "plt.imshow(wordcloud10, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams10 = ngrams(for_ngrams10, 2)\n",
    "\n",
    "bigram_freq10 = Counter(bigrams10)\n",
    "\n",
    "common_bigrams10 = bigram_freq10.most_common(20)\n",
    "\n",
    "common_bigrams10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_bigrams10.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "bigrams10 = list(zip(*common_bigrams10))[0]\n",
    "score2_10 = list(zip(*common_bigrams10))[1]\n",
    "y_pos2_10 = np.arange(len(bigrams10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos2_10, score2_10, align='center')\n",
    "ax.set_yticks(y_pos2_10)\n",
    "ax.set_yticklabels(bigrams10)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring bigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams10 = ngrams(for_ngrams10, 3)\n",
    "\n",
    "trigram_freq10 = Counter(trigrams10)\n",
    "\n",
    "common_trigrams10 = trigram_freq10.most_common(20)\n",
    "\n",
    "common_trigrams10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_trigrams10.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "trigrams10 = list(zip(*common_trigrams10))[0]\n",
    "score3_10 = list(zip(*common_trigrams10))[1]\n",
    "y_pos3_10 = np.arange(len(trigrams10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos3_10, score3_10, align='center')\n",
    "ax.set_yticks(y_pos3_10)\n",
    "ax.set_yticklabels(trigrams10)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring trigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 11 Analysis (Travel for Love)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build segment 11 dataset and reset index\n",
    "segment_11 = comments_master.loc[(comments_master['label_Travel>Travel Type>Honeymoons and Getaways']>0)|\n",
    "                                 (comments_master['label_Events and Attractions>Personal Celebrations and Life Events>Wedding']>0)|\n",
    "                                 (comments_master['label_Travel>Travel Type>Bed & Breakfasts']>0)|\n",
    "                                 (comments_master['label_Family and Relationships>Marriage and Civil Unions']>0)|\n",
    "                                 (comments_master['label_Fine Art>Dance']>0)|\n",
    "                                 (comments_master['label_Style & Fashion>Beauty>Natural and Organic Beauty']>0)].reset_index()\n",
    "\n",
    "segment_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataset for the segment into csv\n",
    "segment_11.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/segment_11.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all comments into one list of words for n-grams analysis\n",
    "for_ngrams11 = []\n",
    "\n",
    "for i in range(len(segment_11['Discussion_cleaned'])):\n",
    "    for x in segment_11['Discussion_cleaned'][i]:\n",
    "        for_ngrams11.append(x)\n",
    "\n",
    "for_ngrams11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_corpus11 = ' '.join(for_ngrams11)\n",
    "\n",
    "main_corpus11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq11 = Counter(for_ngrams11)\n",
    "common_words11 = word_freq11.most_common(20)\n",
    "\n",
    "common_words11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words11.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words1_11 = list(zip(*common_words11))[0]\n",
    "score1_11 = list(zip(*common_words11))[1]\n",
    "y_pos1_11 = np.arange(len(words1_11))\n",
    "\n",
    "commonwords11 = list(words1_11)\n",
    "\n",
    "commonwords11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams11 = ngrams(for_ngrams11, 1)\n",
    "\n",
    "unigram_freq11 = Counter(unigrams11)\n",
    "\n",
    "unigram_freq11.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_for_csv = unigram_freq11\n",
    "all_words_for_csv = word_freq_for_csv.most_common(len(word_freq_for_csv))\n",
    "\n",
    "all_words_for_csv.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words_for_csv = list(zip(*all_words_for_csv))[0]\n",
    "score_for_csv = list(zip(*all_words_for_csv))[1]\n",
    "\n",
    "Term_frequency = pd.DataFrame(data={'term': words_for_csv, 'frequency': score_for_csv})\n",
    "\n",
    "Term_frequency.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/segment_11_unigrams.csv\")\n",
    "\n",
    "Term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos1_11, score1_11, align='center')\n",
    "ax.set_yticks(y_pos1_11)\n",
    "ax.set_yticklabels(words1_11)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring unigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud11 = WordCloud(width=2000, height=1400, max_words=100, max_font_size=300, background_color=\"white\", relative_scaling=.8).generate(main_corpus11)\n",
    "plt.imshow(wordcloud11, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams11 = ngrams(for_ngrams11, 2)\n",
    "\n",
    "bigram_freq11 = Counter(bigrams11)\n",
    "\n",
    "common_bigrams11 = bigram_freq11.most_common(20)\n",
    "\n",
    "common_bigrams11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_bigrams11.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "bigrams11 = list(zip(*common_bigrams11))[0]\n",
    "score2_11 = list(zip(*common_bigrams11))[1]\n",
    "y_pos2_11 = np.arange(len(bigrams11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos2_11, score2_11, align='center')\n",
    "ax.set_yticks(y_pos2_11)\n",
    "ax.set_yticklabels(bigrams11)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring bigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams11 = ngrams(for_ngrams11, 3)\n",
    "\n",
    "trigram_freq11 = Counter(trigrams11)\n",
    "\n",
    "common_trigrams11 = trigram_freq11.most_common(20)\n",
    "\n",
    "common_trigrams11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_trigrams11.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "trigrams11 = list(zip(*common_trigrams11))[0]\n",
    "score3_11 = list(zip(*common_trigrams11))[1]\n",
    "y_pos3_11 = np.arange(len(trigrams11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos3_11, score3_11, align='center')\n",
    "ax.set_yticks(y_pos3_11)\n",
    "ax.set_yticklabels(trigrams11)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring trigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 12 Analysis (Sports Spectators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build segment 12 dataset and reset index\n",
    "segment_12 = comments_master.loc[(comments_master['label_Sports>Soccer']>0)|\n",
    "                                 (comments_master['label_Travel>Travel Type>Road Trips']>0)|\n",
    "                                 (comments_master['label_Sports>Golf']>0)|\n",
    "                                 (comments_master['label_Events and Attractions>Sporting Events']>0)|\n",
    "                                 (comments_master['label_Sports>Auto Racing']>0)|\n",
    "                                 (comments_master['label_Sports>Fishing Sports']>0)].reset_index()\n",
    "\n",
    "segment_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataset for the segment into csv\n",
    "segment_12.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/segment_12.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all comments into one list of words for n-grams analysis\n",
    "for_ngrams12 = []\n",
    "\n",
    "for i in range(len(segment_12['Discussion_cleaned'])):\n",
    "    for x in segment_12['Discussion_cleaned'][i]:\n",
    "        for_ngrams12.append(x)\n",
    "\n",
    "for_ngrams12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_corpus12 = ' '.join(for_ngrams12)\n",
    "\n",
    "main_corpus12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq12 = Counter(for_ngrams12)\n",
    "common_words12 = word_freq12.most_common(20)\n",
    "\n",
    "common_words12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words12.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words1_12 = list(zip(*common_words12))[0]\n",
    "score1_12 = list(zip(*common_words12))[1]\n",
    "y_pos1_12 = np.arange(len(words1_12))\n",
    "\n",
    "commonwords12 = list(words1_12)\n",
    "\n",
    "commonwords12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams12 = ngrams(for_ngrams12, 1)\n",
    "\n",
    "unigram_freq12 = Counter(unigrams12)\n",
    "\n",
    "unigram_freq12.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_for_csv = unigram_freq12\n",
    "all_words_for_csv = word_freq_for_csv.most_common(len(word_freq_for_csv))\n",
    "\n",
    "all_words_for_csv.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words_for_csv = list(zip(*all_words_for_csv))[0]\n",
    "score_for_csv = list(zip(*all_words_for_csv))[1]\n",
    "\n",
    "Term_frequency = pd.DataFrame(data={'term': words_for_csv, 'frequency': score_for_csv})\n",
    "\n",
    "Term_frequency.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/segment_12_unigrams.csv\")\n",
    "\n",
    "Term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos1_12, score1_12, align='center')\n",
    "ax.set_yticks(y_pos1_12)\n",
    "ax.set_yticklabels(words1_12)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring unigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud12 = WordCloud(width=2000, height=1400, max_words=100, max_font_size=300, background_color=\"white\", relative_scaling=.8).generate(main_corpus12)\n",
    "plt.imshow(wordcloud12, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams12 = ngrams(for_ngrams12, 2)\n",
    "\n",
    "bigram_freq12 = Counter(bigrams12)\n",
    "\n",
    "common_bigrams12 = bigram_freq12.most_common(20)\n",
    "\n",
    "common_bigrams12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_bigrams12.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "bigrams12 = list(zip(*common_bigrams12))[0]\n",
    "score2_12 = list(zip(*common_bigrams12))[1]\n",
    "y_pos2_12 = np.arange(len(bigrams12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos2_12, score2_12, align='center')\n",
    "ax.set_yticks(y_pos2_12)\n",
    "ax.set_yticklabels(bigrams12)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring bigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams12 = ngrams(for_ngrams12, 3)\n",
    "\n",
    "trigram_freq12 = Counter(trigrams12)\n",
    "\n",
    "common_trigrams12 = trigram_freq12.most_common(20)\n",
    "\n",
    "common_trigrams12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_trigrams12.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "trigrams12 = list(zip(*common_trigrams12))[0]\n",
    "score3_12 = list(zip(*common_trigrams12))[1]\n",
    "y_pos3_12 = np.arange(len(trigrams12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(y_pos3_12, score3_12, align='center')\n",
    "ax.set_yticks(y_pos3_12)\n",
    "ax.set_yticklabels(trigrams12)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_title('Most occuring trigrams')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_freq_for_csv = trigram_freq12\n",
    "all_words_for_csv = word_freq_for_csv.most_common(len(word_freq_for_csv))\n",
    "\n",
    "all_words_for_csv.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words_for_csv = list(zip(*all_words_for_csv))[0]\n",
    "score_for_csv = list(zip(*all_words_for_csv))[1]\n",
    "\n",
    "Term_frequency = pd.DataFrame(data={'term': words_for_csv, 'frequency': score_for_csv})\n",
    "\n",
    "Term_frequency.to_csv(\"/Users/chrtjen/Desktop/STB_GTT/Segmentation Analysis/segment_12_trigrams.csv\")\n",
    "\n",
    "Term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_columns = [e for e in segment_12.columns if \"label\" in e]\n",
    "df_labels = segment_12[list_of_columns]\n",
    "df_labels_sums = pd.DataFrame(df_labels.astype(bool).sum(axis=0))\n",
    "\n",
    "df_labels_sums.to_csv(\"/Users/chrtjen/Desktop/STB_CN/Segmentation Analysis_China/segment_12_topic_counts.csv\")\n",
    "\n",
    "df_labels_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
